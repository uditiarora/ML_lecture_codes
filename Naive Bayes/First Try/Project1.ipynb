{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test and train data\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data_train = fetch_20newsgroups(subset = 'train')\n",
    "data_test = fetch_20newsgroups(subset = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n",
      "From: v064mb9k@ubvmsd.cc.buffalo.edu (NEIL B. GANDLER)\n",
      "Subject: Need info on 88-89 Bonneville\n",
      "Organization: University at Buffalo\n",
      "Lines: 10\n",
      "News-Software: VAX/VMS VNEWS 1.41\n",
      "Nntp-Posting-Host: ubvmsd.cc.buffalo.edu\n",
      "\n",
      "\n",
      " I am a little confused on all of the models of the 88-89 bonnevilles.\n",
      "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
      "differences are far as features or performance. I am also curious to\n",
      "know what the book value is for prefereably the 89 model. And how much\n",
      "less than book value can you usually get them for. In other words how\n",
      "much are they in demand this time of year. I have heard that the mid-spring\n",
      "early summer is the best time to buy.\n",
      "\n",
      "\t\t\tNeil Gandler\n",
      "\n",
      "From: Rick Miller <rick@ee.uwm.edu>\n",
      "Subject: X-Face?\n",
      "Organization: Just me.\n",
      "Lines: 17\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: 129.89.2.33\n",
      "Summary: Go ahead... swamp me.  <EEP!>\n",
      "\n",
      "I'm not familiar at all with the format of these \"X-Face:\" thingies, but\n",
      "after seeing them in some folks' headers, I've *got* to *see* them (and\n",
      "maybe make one of my own)!\n",
      "\n",
      "I've got \"dpg-view\" on my Linux box (which displays \"uncompressed X-Faces\")\n",
      "and I've managed to compile [un]compface too... but now that I'm *looking*\n",
      "for them, I can't seem to find any X-Face:'s in anyones news headers!  :-(\n",
      "\n",
      "Could you, would you, please send me your \"X-Face:\" header?\n",
      "\n",
      "I *know* I'll probably get a little swamped, but I can handle it.\n",
      "\n",
      "\t...I hope.\n",
      "\n",
      "Rick Miller  <rick@ee.uwm.edu> | <ricxjo@discus.mil.wi.us>   Ricxjo Muelisto\n",
      "Send a postcard, get one back! | Enposxtigu bildkarton kaj vi ricevos alion!\n",
      "          RICK MILLER // 16203 WOODS // MUSKEGO, WIS. 53150 // USA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_train.data[0])\n",
    "print(data_train.data[1])\n",
    "print(data_test.data[0])\n",
    "print(data_test.data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make dictionary of words with their frequency as values\n",
    "def makeDict(data):\n",
    "    \n",
    "    result={}\n",
    "    word_dict={}\n",
    "    #word _dict stores all the words appearing in the text\n",
    "    for i in range(len(data)):\n",
    "        result[i]={}\n",
    "        temp={}\n",
    "        total = 1\n",
    "        for word in data[i].split():\n",
    "            total = total+1\n",
    "            # update in dictionary\n",
    "            if( word in temp.keys()):\n",
    "                temp[word] = temp[word] + 1\n",
    "                \n",
    "            else:\n",
    "                temp[word] = 1\n",
    "            if( word in word_dict.keys()):\n",
    "                word_dict[word] = word_dict[word] + 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "        #store total number of words\n",
    "        temp[\"total_count\"] = total\n",
    "        result[i] = temp\n",
    "    return (result,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create feature table for training data using the dictionary of words created\n",
    "def makeFeatureTable(data_dict,word_dict,y_train):\n",
    "    \n",
    "    # list of frequently occuring words which do not help in classifying the document\n",
    "    remove_words = [\"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \n",
    "\"again\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",    \n",
    "\"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"as\", \"at\", \"be\", \"became\", \"because\", \"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"behind\", \"being\", \"beside\", \"besides\", \"between\", \"beyond\", \"both\", \"but\", \"by\",\"can\", \"cannot\", \"cant\", \"could\", \"couldnt\", \"de\", \"describe\", \"do\", \"done\", \"each\", \"eg\", \"either\", \"else\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"find\",\"for\",\"found\", \"four\", \"from\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"i\", \"ie\", \"if\", \"in\", \"indeed\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\",\"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\",\"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"she\", \"should\",\"since\", \"sincere\",\"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"take\",\"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\",\n",
    "\"this\", \"those\", \"though\", \"through\", \"throughout\",\n",
    "\"thru\", \"thus\", \"to\", \"together\", \"too\", \"toward\", \"towards\",\n",
    "\"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "\"very\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "\"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "\"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \n",
    "\"who\", \"whoever\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "\"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for x in remove_words:\n",
    "        if x in word_dict.keys():\n",
    "            del word_dict[x]\n",
    "    #sort the dictionary according to frequency of words and select most ocuuring words\n",
    "    \n",
    "    sorted_dict = sorted(word_dict.items(),key = operator.itemgetter(1))\n",
    "    n = len(sorted_dict)\n",
    "    feature_list = []      # to store features that are to be considered\n",
    "    for i in range(n-1,n-2501,-1):\n",
    "        feature_list.append(sorted_dict[i][0])\n",
    "    feature_table = np.zeros(shape=(len(y_train),2500))      \n",
    "    \n",
    "    #create feature table \n",
    "    for i in range(len(feature_table)):\n",
    "        total = data_dict[i][\"total_count\"]\n",
    "        for j in range(2500):\n",
    "            if feature_list[j] in data_dict[i].keys():\n",
    "                feature_table[i][j] = data_dict[i][feature_list[j]]\n",
    "    return (feature_table,feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create feature table for testing data\n",
    "def makeFeatureTable_test(data_dict_test,feature_list):\n",
    "    feature_table = np.zeros(shape=(len(data_dict_test),len(feature_list)))\n",
    "    \n",
    "    # make feature table using feature list and dictionary\n",
    "    for i in range(len(data_dict_test)):\n",
    "        total = data_dict_test[i][\"total_count\"]\n",
    "        for j in range(len(feature_list)):\n",
    "            if feature_list[j] in data_dict_test[i].keys():\n",
    "                feature_table[i][j] = data_dict_test[i][feature_list[j]]\n",
    "    return feature_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that creates dictionary of probablities according to training data\n",
    "def fit(X_train, Y_train):\n",
    "    result = {}\n",
    "    class_values = set(Y_train)\n",
    "    for current_class in class_values:\n",
    "        result[current_class] = {}\n",
    "        result[\"total_data\"] = len(Y_train)\n",
    "        current_class_rows = (Y_train == current_class)\n",
    "        X_train_current = X_train[current_class_rows]\n",
    "        Y_train_current = Y_train[current_class_rows]\n",
    "        num_features = X_train.shape[1]\n",
    "        result[current_class][\"total_count\"] = len(Y_train_current)\n",
    "        for j in range(1, num_features + 1):\n",
    "            result[current_class][j] = {}\n",
    "            all_possible_values = set(X_train[:, j - 1])\n",
    "            for current_value in all_possible_values:\n",
    "                result[current_class][j][current_value] = (X_train_current[:, j - 1] == current_value).sum()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(dictionary, x, current_class):\n",
    "    output = np.log(dictionary[current_class][\"total_count\"]) - np.log(dictionary[\"total_data\"])\n",
    "    num_features = len(dictionary[current_class].keys()) - 1;\n",
    "    for j in range(1, num_features + 1):\n",
    "        xj = x[j - 1]\n",
    "        count_current_class_with_value_xj = 1\n",
    "        if(xj in dictionary[current_class][j].keys()):\n",
    "            count_current_class_with_value_xj+= dictionary[current_class][j][xj]\n",
    "        count_current_class = dictionary[current_class][\"total_count\"] + len(dictionary[current_class][j].keys())\n",
    "        current_xj_probablity = np.log(count_current_class_with_value_xj) - np.log(count_current_class)\n",
    "        output = output + current_xj_probablity\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSinglePoint(dictionary, x):\n",
    "    classes = dictionary.keys()\n",
    "    best_p = -1000\n",
    "    best_class = -1\n",
    "    first_run = True\n",
    "    \n",
    "    for current_class in classes:\n",
    "        if (current_class == \"total_data\"):\n",
    "            continue\n",
    "        p_current_class = probability(dictionary, x, current_class)\n",
    "        if (first_run or p_current_class > best_p):\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "        first_run = False\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dictionary, X_test):\n",
    "    y_pred = []\n",
    "    for x in X_test:\n",
    "        x_class = predictSinglePoint(dictionary, x)\n",
    "        y_pred.append(x_class)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "for i in range(len(data_train.data)):\n",
    "    line = data_train.data[i]\n",
    "    list1 = line.split(\"\\n\")\n",
    "    string=\"\"\n",
    "    index=0\n",
    "    while(\":\" in list1[index]):\n",
    "        index += 1\n",
    "    for j in range(index,len(list1)):\n",
    "        list1[j] = list1[j].replace(\",\",\" \")\n",
    "        list1[j] = list1[j].replace(\":\",\" \")\n",
    "        list1[j] = list1[j].replace(\"'\",\" \")\n",
    "        list1[j] = list1[j].replace('\"',\" \")\n",
    "        list1[j] = list1[j].replace(\"!\",\" \")\n",
    "        list1[j] = list1[j].replace(\"?\",\" \")\n",
    "        list1[j] = list1[j].replace(\"(\",\" \")\n",
    "        list1[j] = list1[j].replace(\")\",\" \")\n",
    "        list1[j] = list1[j].replace(\"-\",\" \")\n",
    "        list1[j] = list1[j].replace(\"<\",\" \")\n",
    "        list1[j] = list1[j].replace(\">\",\" \")\n",
    "\n",
    "        string += list1[j]\n",
    "    x_train.append(string)\n",
    "x_train = np.array(x_train)\n",
    "y_train = data_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "for i in range(len(data_test.data)):\n",
    "    line = data_test.data[i]\n",
    "    list1 = line.split(\"\\n\")\n",
    "    string=\"\"\n",
    "    index=0\n",
    "    while(\":\" in list1[index]):\n",
    "        index += 1\n",
    "    for j in range(index,len(list1)):\n",
    "        list1[j] = list1[j].replace(\",\",\" \")\n",
    "        list1[j] = list1[j].replace(\":\",\" \")\n",
    "        list1[j] = list1[j].replace(\"'\",\" \")\n",
    "        list1[j] = list1[j].replace('\"',\" \")\n",
    "        list1[j] = list1[j].replace(\"!\",\" \")\n",
    "        list1[j] = list1[j].replace(\"?\",\" \")\n",
    "        list1[j] = list1[j].replace(\"(\",\" \")\n",
    "        list1[j] = list1[j].replace(\")\",\" \")\n",
    "        list1[j] = list1[j].replace(\"-\",\" \")\n",
    "        list1[j] = list1[j].replace(\"<\",\" \")\n",
    "        list1[j] = list1[j].replace(\">\",\" \")\n",
    "\n",
    "        string += list1[j]\n",
    "    x_test.append(string)\n",
    "x_test = np.array(x_test)\n",
    "y_test = data_test.target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict,word_dict = makeDict(x_train)\n",
    "data_dict_test,temp = makeDict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_table,feature_list = makeFeatureTable(data_dict,word_dict,y_train)\n",
    "feature_table_test = makeFeatureTable_test(data_dict_test,feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5892193308550185\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(feature_table,y_train)\n",
    "y_pred = clf.predict(feature_table_test)\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = fit(feature_table,y_train)\n",
    "y_pred = predict(dictionary,feature_table_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4711895910780669\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
