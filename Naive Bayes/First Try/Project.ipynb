{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test and train data\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data_train = fetch_20newsgroups(subset = 'train')\n",
    "data_test = fetch_20newsgroups(subset = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: jgreen@amber (Joe Green)\n",
      "Subject: Re: Weitek P9000 ?\n",
      "Organization: Harris Computer Systems Division\n",
      "Lines: 14\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: amber.ssd.csd.harris.com\n",
      "X-Newsreader: TIN [version 1.1 PL9]\n",
      "\n",
      "Robert J.C. Kyanko (rob@rjck.UUCP) wrote:\n",
      "> abraxis@iastate.edu writes in article <abraxis.734340159@class1.iastate.edu>:\n",
      "> > Anyone know about the Weitek P9000 graphics chip?\n",
      "> As far as the low-level stuff goes, it looks pretty nice.  It's got this\n",
      "> quadrilateral fill command that requires just the four points.\n",
      "\n",
      "Do you have Weitek's address/phone number?  I'd like to get some information\n",
      "about this chip.\n",
      "\n",
      "--\n",
      "Joe Green\t\t\t\tHarris Corporation\n",
      "jgreen@csd.harris.com\t\t\tComputer Systems Division\n",
      "\"The only thing that really scares me is a person with no sense of humor.\"\n",
      "\t\t\t\t\t\t-- Jonathan Winters\n",
      "\n",
      "From: v064mb9k@ubvmsd.cc.buffalo.edu (NEIL B. GANDLER)\n",
      "Subject: Need info on 88-89 Bonneville\n",
      "Organization: University at Buffalo\n",
      "Lines: 10\n",
      "News-Software: VAX/VMS VNEWS 1.41\n",
      "Nntp-Posting-Host: ubvmsd.cc.buffalo.edu\n",
      "\n",
      "\n",
      " I am a little confused on all of the models of the 88-89 bonnevilles.\n",
      "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
      "differences are far as features or performance. I am also curious to\n",
      "know what the book value is for prefereably the 89 model. And how much\n",
      "less than book value can you usually get them for. In other words how\n",
      "much are they in demand this time of year. I have heard that the mid-spring\n",
      "early summer is the best time to buy.\n",
      "\n",
      "\t\t\tNeil Gandler\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_train.data[3])\n",
    "print(data_test.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make dictionary of words with their frequency as values\n",
    "def makeDict(data):\n",
    "    \n",
    "    result={}\n",
    "    word_dict={}\n",
    "    #word _dict stores all the words appearing in the text\n",
    "    for i in range(len(data)):\n",
    "        result[i]={}\n",
    "        temp={}\n",
    "        total = 1\n",
    "        for word in data[i].split():\n",
    "            #remove unwanted characters\n",
    "            word = word.strip()\n",
    "            word = word.strip(\",\")\n",
    "            word = word.strip(\":\")\n",
    "            word = word.strip(\"'\")\n",
    "            word = word.strip(\".\")\n",
    "            word = word.strip(\"?\")\n",
    "            word = word.strip(\"!\")\n",
    "            word = word.strip(\"(\")\n",
    "            word = word.strip(\")\")\n",
    "            word = word.strip('\"')\n",
    "            total = total+1\n",
    "            # update in dictionary\n",
    "            if( word in temp.keys()):\n",
    "                temp[word] = temp[word] + 1\n",
    "                \n",
    "            else:\n",
    "                temp[word] = 1\n",
    "            if( word in word_dict.keys()):\n",
    "                word_dict[word] = word_dict[word] + 1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "        #store total number of words\n",
    "        temp[\"total_count\"] = total\n",
    "        result[i] = temp\n",
    "    return (result,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create feature table for training data using the dictionary of words created\n",
    "def makeFeatureTable(data_dict,word_dict,y_train):\n",
    "    \n",
    "    # list of frequently occuring words which do not help in classifying the document\n",
    "    remove_words = [\"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \n",
    "\"again\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",    \n",
    "\"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"as\", \"at\", \"be\", \"became\", \"because\", \"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"behind\", \"being\", \"beside\", \"besides\", \"between\", \"beyond\", \"both\", \"but\", \"by\",\"can\", \"cannot\", \"cant\", \"could\", \"couldnt\", \"de\", \"describe\", \"do\", \"done\", \"each\", \"eg\", \"either\", \"else\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"find\",\"for\",\"found\", \"four\", \"from\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"i\", \"ie\", \"if\", \"in\", \"indeed\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\",\"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\",\"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"she\", \"should\",\"since\", \"sincere\",\"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"take\",\"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\",\n",
    "\"this\", \"those\", \"though\", \"through\", \"throughout\",\n",
    "\"thru\", \"thus\", \"to\", \"together\", \"too\", \"toward\", \"towards\",\n",
    "\"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "\"very\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "\"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "\"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \n",
    "\"who\", \"whoever\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "\"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for x in remove_words:\n",
    "        if x in word_dict.keys():\n",
    "            del word_dict[x]\n",
    "    #sort the dictionary according to frequency of words and select most ocuuring words\n",
    "    \n",
    "    sorted_dict = sorted(word_dict.items(),key = operator.itemgetter(1))\n",
    "    n = len(sorted_dict)\n",
    "    feature_list = []      # to store features that are to be considered\n",
    "    for i in range(n-1,n-1001,-1):\n",
    "        feature_list.append(sorted_dict[i][0])\n",
    "    feature_table = np.zeros(shape=(len(y_train),1000))      \n",
    "    \n",
    "    #create feature table \n",
    "    for i in range(len(feature_table)):\n",
    "        total = data_dict[i][\"total_count\"]\n",
    "        for j in range(1000):\n",
    "            if feature_list[j] in data_dict[i].keys():\n",
    "                feature_table[i][j] = data_dict[i][feature_list[j]]\n",
    "    return (feature_table,feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create feature table for testing data\n",
    "def makeFeatureTable_test(data_dict_test,feature_list):\n",
    "    feature_table = np.zeros(shape=(len(data_dict_test),len(feature_list)))\n",
    "    \n",
    "    # make feature table using feature list and dictionary\n",
    "    for i in range(len(data_dict_test)):\n",
    "        total = data_dict_test[i][\"total_count\"]\n",
    "        for j in range(len(feature_list)):\n",
    "            if feature_list[j] in data_dict_test[i].keys():\n",
    "                feature_table[i][j] = data_dict_test[i][feature_list[j]]\n",
    "    return feature_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that creates dictionary of probablities according to training data\n",
    "def fit(X_train, Y_train):\n",
    "    result = {}\n",
    "    class_values = set(Y_train)\n",
    "    for current_class in class_values:\n",
    "        result[current_class] = {}\n",
    "        result[\"total_data\"] = len(Y_train)\n",
    "        current_class_rows = (Y_train == current_class)\n",
    "        X_train_current = X_train[current_class_rows]\n",
    "        Y_train_current = Y_train[current_class_rows]\n",
    "        num_features = X_train.shape[1]\n",
    "        result[current_class][\"total_count\"] = len(Y_train_current)\n",
    "        for j in range(1, num_features + 1):\n",
    "            result[current_class][j] = {}\n",
    "            all_possible_values = set(X_train[:, j - 1])\n",
    "            for current_value in all_possible_values:\n",
    "                result[current_class][j][current_value] = (X_train_current[:, j - 1] == current_value).sum()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(dictionary, x, current_class):\n",
    "    output = np.log(dictionary[current_class][\"total_count\"]) - np.log(dictionary[\"total_data\"])\n",
    "    num_features = len(dictionary[current_class].keys()) - 1;\n",
    "    for j in range(1, num_features + 1):\n",
    "        xj = x[j - 1]\n",
    "        count_current_class_with_value_xj = 1\n",
    "        if(xj in dictionary[current_class][j].keys()):\n",
    "            count_current_class_with_value_xj+= dictionary[current_class][j][xj]\n",
    "        count_current_class = dictionary[current_class][\"total_count\"] + len(dictionary[current_class][j].keys())\n",
    "        current_xj_probablity = np.log(count_current_class_with_value_xj) - np.log(count_current_class)\n",
    "        output = output + current_xj_probablity\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSinglePoint(dictionary, x):\n",
    "    classes = dictionary.keys()\n",
    "    best_p = -1000\n",
    "    best_class = -1\n",
    "    first_run = True\n",
    "    \n",
    "    for current_class in classes:\n",
    "        if (current_class == \"total_data\"):\n",
    "            continue\n",
    "        p_current_class = probability(dictionary, x, current_class)\n",
    "        if (first_run or p_current_class > best_p):\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "        first_run = False\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dictionary, X_test):\n",
    "    y_pred = []\n",
    "    for x in X_test:\n",
    "        x_class = predictSinglePoint(dictionary, x)\n",
    "        y_pred.append(x_class)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_train.data\n",
    "y_train = data_train.target\n",
    "x_test = data_test.data\n",
    "y_test = data_test.target\n",
    "data_dict,word_dict = makeDict(x_train)\n",
    "data_dict_test,temp = makeDict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_table,feature_list = makeFeatureTable(data_dict,word_dict,y_train)\n",
    "feature_table_test = makeFeatureTable_test(data_dict_test,feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5586829527349974\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(feature_table,y_train)\n",
    "y_pred = clf.predict(feature_table_test)\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = fit(feature_table,y_train)\n",
    "y_pred = predict(dictionary,feature_table_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5169941582580988\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
