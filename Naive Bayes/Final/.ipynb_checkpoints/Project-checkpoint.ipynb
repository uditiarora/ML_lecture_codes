{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import operator as op\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the list of classes from the list directory in 20_newsgroup dataset\n",
    "list_of_categories = os.listdir('./20_newsgroups')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of stopwords from a file which has already been saved\n",
    "stop_file = open('stopwords_en.txt','r')\n",
    "stop_words = stop_file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-57c7b4229c91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./20_newsgroups/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# accessing inner folders in all types of news\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./20_newsgroups/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# accessing files in inner folders\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mtextfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\w+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtextfile\u001b[0m\u001b[1;33m)\u001b[0m      \u001b[1;31m# making object of re\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\_bootlocale.py\u001b[0m in \u001b[0;36mgetpreferredencoding\u001b[1;34m(do_setlocale)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"win\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mgetpreferredencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_locale\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getdefaultlocale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dictionary={}      # dictionary to store words with their frequency\n",
    "\n",
    "for folder in list_of_categories :\n",
    "    files = os.listdir('./20_newsgroups/'+folder)  # accessing inner folders in all types of news\n",
    "    for file in files:\n",
    "        f = open('./20_newsgroups/'+folder+'/'+file,'r')   # accessing files in inner folders\n",
    "        textfile = f.read()\n",
    "        tokens = re.compile('\\w+').findall(textfile)      # making object of re\n",
    "        \n",
    "        for token in tokens:\n",
    "            if(token.isalpha() and len(token)>2 and token.lower() not in stop_words):\n",
    "                if(token.lower() not in dictionary.keys()):\n",
    "                    dictionary[token.lower()] = 1\n",
    "                else:\n",
    "                    dictionary[token.lower()]+=1\n",
    "listoftuples = sorted(dictionary.items(),key=op.itemgetter(1),reverse=True)    # sorted on basis of value in decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vocab_list = listoftuples[:1000] # top 1000 words chosen\n",
    "final_vocab = {}\n",
    "#making a dictionary containing only top 1000 words\n",
    "for t in final_vocab_list:\n",
    "    final_vocab[t[0]] = t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing these values of dictionary in a file named vocab_file\n",
    "fp = open('vocab_file','w')\n",
    "for item in final_vocab.items():\n",
    "    fp.write(str(item)+'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the header of the dataframe that we are going to use ahead\n",
    "# we store this dataframe in file as well\n",
    "df=pd.DataFrame()\n",
    "for feature in final_vocab.keys():\n",
    "    df[feature] = 0\n",
    "df['target'] = 0\n",
    "df.to_csv('df_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# constructing our feature table\n",
    "# we store it in our dataframe df\n",
    "for folder in list_of_categories:\n",
    "    files = os.listdir('./20_newsgroups/'+folder)\n",
    "    for file in files:\n",
    "        words = []\n",
    "        count = 0\n",
    "        current_count = 0\n",
    "        f=open('./20_newsgroups/'+folder+'/'+file,'r')\n",
    "        textfile = f.read()\n",
    "        tokens = re.compile('\\w+').findall(textfile)\n",
    "        \n",
    "        for token in tokens:\n",
    "            if(token.isalpha() and len(token)>2 and token.lower() not in stop_words):\n",
    "                words.append(token.lower())                     # words appended in words list as many times as they occur\n",
    "                \n",
    "        row=[]\n",
    "        for feature in final_vocab.keys():\n",
    "            for word in words:\n",
    "                if(feature == word):\n",
    "                    count += 1\n",
    "                    \n",
    "            row.append(count)  # appending count of each word in row\n",
    "            count = 0\n",
    "            \n",
    "        row.append(folder)  # adding type of news group at the end\n",
    "        df.loc[len(df)] = row   # appending in the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving this feature table in our file to access it later without doing the whole work again\n",
    "df.to_csv('feature_table.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will access the data from the files that we have stored after filtering the data.\n",
    "We first try inbuilt MultinomialNb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access x and y values\n",
    "X = df.iloc[:,:df.shape[1]-1].values\n",
    "Y = df.iloc[:,-1].values\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=MultinomialNB()\n",
    "clf.fit(xtrain,ytrain)\n",
    "ypred1 = clf.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(ytest,ypred1))\n",
    "print(confusion_matrix(ytest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(ytest,ypred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ypred1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move on to the next section in which we make predictions on Mini newsgroups after training our data using the 20 newsgroups. We had stored the trained data in a file. We can now directly access it to continue further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature table\n",
    "df = pd.read_csv('feature_table.csv')\n",
    "dictionary = {}\n",
    "# get words that have been used in the feature table\n",
    "\n",
    "fp = open('vocab_file','r+')\n",
    "l = fp.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creatre a dictionary containing (word, frequency) pairs using the data read above from vocab_file\n",
    "key=\"\"\n",
    "value=-1\n",
    "for s in l:\n",
    "    ts=s.strip().split(',')\n",
    "    for i in ts:\n",
    "        templist=[letter for letter in i if letter not in string.punctuation]\n",
    "        tempstring=''.join(templist).strip()\n",
    "        if(tempstring.isalpha()):\n",
    "            dictionary[tempstring]=value\n",
    "            key=tempstring\n",
    "        elif(tempstring.isnumeric()):\n",
    "            dictionary[key]=int(tempstring)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all classes \n",
    "ls = os.listdir('./mini_newsgroups/')\n",
    "dft = pd.DataFrame()          # test data frame\n",
    "for feature in dictionary.keys():\n",
    "    dft[feature] = 0\n",
    "dft['target'] = 0\n",
    "fsw = open('stopwords_en.txt','r')\n",
    "stop_words=fsw.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test data\n",
    "# same as done before but now on mini newsgroups\n",
    "for folder in ls:\n",
    "    files=os.listdir('./mini_newsgroups/'+folder)\n",
    "    for file in files:\n",
    "        words = []\n",
    "        count = 0\n",
    "        current_count = 0\n",
    "        f = open('./mini_newsgroups/'+folder+'/'+file,'r')\n",
    "        textfile = f.read()\n",
    "        tokens = re.compile('\\w+').findall(textfile)\n",
    "        \n",
    "        for token in tokens:\n",
    "            if(token.isalpha() and len(token)>2 and token.lower() not in stop_words):\n",
    "                words.append(token.lower())\n",
    "                \n",
    "        row = []\n",
    "        for feature in dictionary.keys():\n",
    "            for word in words:\n",
    "                if(feature == word):\n",
    "                    count += 1\n",
    "                    \n",
    "            row.append(count)\n",
    "            count = 0\n",
    "            \n",
    "        row.append(folder)\n",
    "        dft.loc[len(dft)] = row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save testing dataframe in a file\n",
    "dft.to_csv('testing_data_frame.csv')\n",
    "del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = df.iloc[:,:df.shape[1]-1]\n",
    "ytrain = df.iloc[:,-1]\n",
    "xtest = dft.iloc[:,:dft.shape[1]-1]\n",
    "ytest = dft.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=MultinomialNB()\n",
    "clf.fit(xtrain,ytrain)\n",
    "ypred2 = clf.predict(xtest)\n",
    "print(accuracy_score(ytest,ypred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(ytest,ypred2))\n",
    "print(confusion_matrix(ytest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ypred2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will code our own naive bayes and predict results using these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same process as done above\n",
    "df = pd.read_csv('feature_table.csv') # read training and testing data\n",
    "dftest = pd.read_csv('testing_data_frame.csv')\n",
    "dictionary={}\n",
    "fp = open('vocab_file','r+')\n",
    "l = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key=\"\"\n",
    "value=-1\n",
    "for s in l:\n",
    "    ts=s.strip().split(',')\n",
    "    for i in ts:\n",
    "        templist=[letter for letter in i if letter not in string.punctuation]\n",
    "        tempstring=''.join(templist).strip()\n",
    "        if(tempstring.isalpha()):\n",
    "            dictionary[tempstring]=value\n",
    "            key=tempstring\n",
    "        elif(tempstring.isnumeric()):\n",
    "            dictionary[key]=int(tempstring)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Unnamed: 0']\n",
    "del dftest['Unnamed: 0']\n",
    "no_of_words=len(dictionary)\n",
    "# get data \n",
    "xtrain = df.iloc[:,:df.shape[1]-1]\n",
    "ytrain = df.iloc[:,-1]\n",
    "xtest = dftest.iloc[:,:dftest.shape[1]-1]\n",
    "ytest = dftest.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit function \n",
    "def fit(x_train,y_train):\n",
    "    result={}\n",
    "    classes=set(y_train)\n",
    "    result['total_docs']=x_train.shape[0]\n",
    "\n",
    "    for current_class in classes:\n",
    "        result[current_class] = {}\n",
    "        current_class_rows = (current_class==y_train)\n",
    "        x_train_current = x_train[current_class_rows]\n",
    "        y_train_current = y_train[current_class_rows]     \n",
    "        result[current_class]['no_of_docs'] = x_train_current.shape[0]\n",
    "        num_features = x_train_current.shape[1]\n",
    "        total_words_in_current_class = 0\n",
    "        for j in range(num_features):\n",
    "            freq_of_a_word_in_current_class = x_train_current.iloc[:,j].sum()\n",
    "            result[current_class][j] = freq_of_a_word_in_current_class\n",
    "            total_words_in_current_class += freq_of_a_word_in_current_class\n",
    "            \n",
    "        result[current_class]['total_words_in_class'] = total_words_in_current_class\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(dictionary_count,x,current_class):\n",
    "    p_y_equals_c = np.log(dictionary_count[current_class]['no_of_docs'])-np.log(dictionary_count['total_docs'])\n",
    "    p_wi_in_doc = dictionary=0\n",
    "    rangeloop = len(dictionary_count[current_class].keys())-2\n",
    "    for i in range(rangeloop):\n",
    "        wi = x[i]\n",
    "        if(wi == 0):\n",
    "            probrv = 0\n",
    "        else:\n",
    "            num = (dictionary_count[current_class][i] + 1)\n",
    "            den = dictionary_count[current_class]['total_words_in_class'] + rangeloop\n",
    "            probrv = np.log(num)-np.log(den)\n",
    "        \n",
    "        p_wi_in_doc += probrv     \n",
    "    return p_wi_in_doc+p_y_equals_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSinglePoint(dictionary_count,x):\n",
    "    classes = dictionary_count.keys()\n",
    "    best_p = -1000\n",
    "    best_class=''\n",
    "    fr = True\n",
    "    for current_class in classes:\n",
    "        if(current_class == 'total_docs'):\n",
    "            continue\n",
    "        p = probability(dictionary_count,x,current_class)\n",
    "        if(fr or p>best_p):\n",
    "            best_p = p\n",
    "            best_class = current_class\n",
    "        fr = False\n",
    "        \n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict function\n",
    "def predict(dictionary_count,x_test_data):\n",
    "    ypredrv = []\n",
    "    for i in range(x_test_data.shape[0]):\n",
    "        x = x_test_data.iloc[i]\n",
    "        x_class = predictSinglePoint(dictionary_count,x)\n",
    "        ypredrv.append(x_class)\n",
    "    return ypredrv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary_count = fit(xtrain,ytrain)\n",
    "ypred3 = predict(dictionary_count,xtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(ytest,ypred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(ytest,ypred3))\n",
    "print(confusion_matrix(ytest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ypred3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
